# ADCC Analysis Engine v0.6.0-alpha.6 - Cursor Rules

## Project Overview
This is a Python-based ADCC analysis engine for tracking athlete ratings, generating leaderboards, and managing tournament data. The system processes Smoothcomp data, calculates Glicko-2 ratings, and provides a web interface for querying athlete information.

## Core Architecture Principles
- **Modular Design**: Each module should be self-contained with clear interfaces
- **ID-Based Operations**: All data operations use unique IDs (Athlete_ID, Event_ID, Division_ID, etc.)
- **Chronological Processing**: Data is processed in chronological order with state management
- **Three-Stream Processing**: Youth, Adult, and Masters data are processed separately
- **Extensive Logging**: Every operation should be logged for debugging and audit purposes

## Python Coding Standards

### Code Style
- Follow **PEP 8** strictly
- Use **type hints** for all function parameters and return values
- Write **comprehensive docstrings** using Google Style format
- Keep functions focused and under 50 lines when possible
- Use descriptive variable names (avoid abbreviations)

### Import Organization
```python
# Standard library imports
import os
import logging
from typing import Dict, List, Optional, Union

# Third-party imports
import pandas as pd
import numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# Local imports
from src.core.constants import AGE_CLASSES, WIN_WEIGHTS
from src.utils.logger import get_logger
```

### Error Handling
- Use specific exception types, not generic `Exception`
- Log errors with context information
- Implement graceful degradation where possible
- Create custom exceptions in `src/core/exceptions.py`

### Logging Standards
```python
import logging
logger = logging.getLogger(__name__)

def process_athlete_data(athlete_id: str, data: pd.DataFrame) -> Dict:
    """Process athlete data and return results."""
    logger.info(f"Processing athlete {athlete_id} with {len(data)} records")
    try:
        # Processing logic
        logger.debug(f"Data sample: {data.head().to_dict()}")
        return result
    except Exception as e:
        logger.error(f"Failed to process athlete {athlete_id}: {str(e)}")
        raise
```

## File Structure Guidelines

### Module Organization
- Each module should have a clear, single responsibility
- Use `__init__.py` files to define public interfaces
- Keep related functionality together
- Follow the established directory structure from the architecture

### File Naming
- Use snake_case for all Python files
- Use descriptive names that indicate purpose
- Prefix utility files appropriately (e.g., `glicko_engine.py`, `file_handler.py`)

## Data Processing Guidelines

### DataFrame Operations
- Always validate input data before processing
- Use explicit column names, not positional indexing
- Handle missing data explicitly
- Log data transformations for debugging

```python
def normalize_athlete_names(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize athlete names in the dataframe."""
    logger.info(f"Normalizing names for {len(df)} records")
    
    # Validate required columns
    required_cols = ['firstname', 'lastname']
    if not all(col in df.columns for col in required_cols):
        raise ValueError(f"Missing required columns: {required_cols}")
    
    # Process names
    df['normalized_name'] = df['firstname'].str.strip() + ' ' + df['lastname'].str.strip()
    df['normalized_name'] = df['normalized_name'].str.title()
    
    logger.debug(f"Name normalization complete. Sample: {df['normalized_name'].head().tolist()}")
    return df
```

### ID Generation
- Always check for existing IDs before generating new ones
- Use consistent ID formats (A123456, E123456, D-Y16U-M-ADV-W70KG)
- Log ID generation for audit purposes

## Git Workflow and Actions

### Branching Strategy
- **main**: Production-ready code
- **develop**: Integration branch for features
- **feature/**: Individual features (e.g., `feature/glicko-implementation`)
- **hotfix/**: Critical bug fixes
- **release/**: Release preparation

### Commit Message Standards
Use conventional commit format:
```
type(scope): description

[optional body]

[optional footer]
```

Types: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`
Scopes: `api`, `ui`, `data`, `glicko`, `auth`, `webhook`, etc.

Examples:
```
feat(data): implement athlete ID generation system
fix(glicko): correct rating calculation for submission wins
docs(api): add endpoint documentation for athlete profiles
refactor(ui): improve leaderboard filtering performance
```

### Git Actions to Include
- **Pre-commit hooks**: Run linting, type checking, and tests
- **Branch protection**: Require PR reviews for main/develop
- **Automated testing**: Run tests on every push
- **Code quality checks**: Enforce style guidelines
- **Dependency updates**: Monitor security vulnerabilities

### Recommended Git Commands
```bash
# Feature development
git checkout -b feature/new-feature
git add .
git commit -m "feat(scope): implement new feature"
git push origin feature/new-feature

# Before merging
git fetch origin
git rebase origin/develop
git push --force-with-lease

# Release preparation
git checkout -b release/v0.6.0-alpha.7
git tag -a v0.6.0-alpha.7 -m "Release version 0.6.0-alpha.7"
git push origin v0.6.0-alpha.7
```



## Configuration Management

### Environment Variables
- Use `.env` files for local development
- Never commit sensitive data
- Validate configuration on startup
- Provide sensible defaults

### Constants Usage
- Use `src/core/constants.py` for system-wide constants
- Use `config/settings.py` for environment-specific settings
- Document all configuration options

## Performance Guidelines

### Data Processing
- Use pandas efficiently (vectorized operations)
- Implement pagination for large datasets
- Cache frequently accessed data
- Monitor memory usage

### Web UI
- Implement lazy loading for athlete profiles
- Use efficient database queries
- Optimize static asset delivery
- Implement proper caching headers

## Security Guidelines

### Input Validation
- Validate all user inputs
- Sanitize data before processing
- Use parameterized queries
- Implement rate limiting

### Authentication
- Use secure session management
- Implement proper password hashing
- Log authentication attempts
- Use HTTPS in production

## Documentation Standards

### Code Documentation
- Document all public functions and classes
- Include usage examples
- Document exceptions and error conditions
- Keep documentation up to date

### API Documentation
- Use FastAPI's automatic documentation
- Include request/response examples
- Document error codes and messages
- Keep OpenAPI spec updated

## Debugging Guidelines

### Logging Strategy
- Use appropriate log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- Include context in log messages
- Log data transformations for debugging
- Use structured logging for complex data

### Error Handling
- Catch specific exceptions
- Provide meaningful error messages
- Include stack traces for debugging
- Implement graceful degradation

## Deployment Guidelines

### Environment Setup
- Use virtual environments
- Pin dependency versions
- Document deployment process
- Use environment-specific configurations

### Monitoring
- Implement health checks
- Monitor application performance
- Set up error alerting
- Track user activity

## Project-Specific Rules

### Data Processing Pipeline
- Always validate input data before processing
- Process data in chronological order
- Maintain data integrity throughout pipeline
- Implement rollback capabilities

### Glicko-2 Implementation
- Follow the mathematical specification exactly
- Test with known values
- Implement proper period finalization
- Handle provisional vs. final ratings

### Web UI Development
- Use responsive design principles
- Implement proper accessibility features
- Test across different browsers
- Optimize for mobile devices

## AI Assistant Guidelines

### When Working with AI
- Provide complete context for complex tasks
- Break down large features into smaller tasks
- Review and test AI-generated code thoroughly
- Document any AI-generated solutions

### Code Review Process
- Review all AI-generated code
- Test functionality thoroughly
- Ensure adherence to project standards
- Update documentation as needed

## Quality Assurance

### Code Review Checklist
- [ ] Follows PEP 8 style guidelines
- [ ] Includes type hints
- [ ] Has comprehensive docstrings
- [ ] Includes appropriate logging
- [ ] Handles errors gracefully
- [ ] Includes tests
- [ ] Updates documentation

### Performance Checklist
- [ ] Efficient data processing
- [ ] Proper memory usage
- [ ] Fast response times
- [ ] Scalable design
- [ ] Caching implemented where appropriate

## Emergency Procedures

### Rollback Process
- Keep backups of working versions
- Document rollback procedures
- Test rollback process regularly
- Have emergency contacts documented

### Data Recovery
- Implement data validation
- Keep audit logs
- Test recovery procedures
- Document recovery steps

---

## Database & File Management Rules

### Parquet File Optimization
- Use appropriate compression (snappy for speed, gzip for size)
- Partition large datasets by date or event
- Optimize column order for common queries
- Monitor file sizes and implement cleanup strategies

### JSON File Structure Validation
- Validate JSON schema before writing
- Use consistent indentation (2 spaces)
- Implement versioning for schema changes
- Backup JSON files before major updates

### Backup and Recovery Procedures
- Automated daily backups of critical data
- Test restore procedures monthly
- Keep multiple backup locations
- Document recovery time objectives (RTO)

## Web UI Specific Rules

### Frontend Framework Guidelines
- Use **vanilla JavaScript** for simplicity and performance
- Implement progressive enhancement
- Follow BEM methodology for CSS
- Use semantic HTML elements
- No build process required - direct browser execution

### CSS/JS Organization Standards
```css
/* Component-based organization */
.athlete-profile { }
.athlete-profile__name { }
.athlete-profile__stats { }
.athlete-profile--highlighted { }
```

```javascript
// Module pattern for JavaScript
const AthleteProfile = {
    init() { },
    render(data) { },
    update(newData) { }
};
```

### Responsive Design Requirements
- Mobile-first approach
- Breakpoints: 320px, 768px, 1024px, 1440px
- Test on real devices, not just browser dev tools
- Ensure touch targets are at least 44px







## Documentation Rules

### README Structure Requirements
```markdown
# ADCC Analysis Engine v0.6.0-alpha.6

## Overview
Brief description of the project

## Quick Start
Installation and setup instructions

## Architecture
Link to architecture document

## API Documentation
Link to API docs

## Development
Development setup and guidelines

## Deployment
Deployment instructions

## Contributing
Contribution guidelines
```

### API Documentation Standards
- Use FastAPI's automatic OpenAPI generation
- Include request/response examples
- Document all error codes and messages
- Provide interactive testing interface

### Code Comment Guidelines
```python
def calculate_glicko_rating(
    current_rating: float,
    current_rd: float,
    match_results: List[MatchResult]
) -> Tuple[float, float]:
    """
    Calculate new Glicko-2 rating based on match results.
    
    Args:
        current_rating: Current Glicko rating
        current_rd: Current rating deviation
        match_results: List of match results to process
        
    Returns:
        Tuple of (new_rating, new_rd)
        
    Raises:
        ValueError: If match_results is empty
        GlickoError: If calculation fails
        
    Example:
        >>> calculate_glicko_rating(1500, 350, [win, loss])
        (1520.5, 320.1)
    """
```



### Memory Management
- Monitor memory usage in production
- Implement garbage collection optimization
- Use generators for large datasets
- Profile memory usage regularly

### Caching Strategy
```python
# In-memory caching for frequently accessed data
CACHE_TTL = {
    'athlete_profile': 3600,      # 1 hour
    'leaderboard': 1800,          # 30 minutes
    'event_data': 7200,           # 2 hours
    'glicko_scores': 300          # 5 minutes
}

# File-based caching for processed data
PROCESSED_DATA_CACHE = {
    'parquet_files': 'data/processed/cache/',
    'json_dictionaries': 'data/datastore/cache/',
    'max_cache_size': '1GB'
}
```

## Security Best Practices

### Input Sanitization
```python
import html
import re

def sanitize_input(user_input: str) -> str:
    """Sanitize user input to prevent XSS and injection."""
    # Remove HTML tags
    cleaned = html.escape(user_input)
    # Remove potentially dangerous characters
    cleaned = re.sub(r'[<>"\']', '', cleaned)
    return cleaned.strip()
```

### Rate Limiting Implementation
```python
from fastapi import HTTPException
import time

RATE_LIMITS = {
    'public': {'requests': 100, 'window': 3600},    # 100 requests per hour
    'admin': {'requests': 1000, 'window': 3600},    # 1000 requests per hour
    'developer': {'requests': 5000, 'window': 3600} # 5000 requests per hour
}
```

### Session Security
- Use **session-based authentication** (not JWT)
- Implement secure session cookies
- Set appropriate session timeouts (1 hour for admin, 30 minutes for developer)
- Log all authentication attempts
- Use HTTPS in production

## Testing Best Practices

### Test Coverage Requirements
- **Minimum 80% code coverage** for all code
- **100% coverage** for critical functions (Glicko calculations, data validation, ID generation)
- **Test all error conditions** and edge cases
- **Include integration tests** for complete workflows
- **Exclude 20%** from coverage: configuration loading, logging setup, performance optimization code

### Coverage Strategy Breakdown
```python
# Unit Tests (60% of coverage)
def test_generate_athlete_id_new_athlete():
    """Test ID generation for new athlete."""
    smoothcomp_id = 123456
    result = generate_athlete_id(smoothcomp_id)
    assert result == "A123456"

def test_glicko_calculation_submission_win():
    """Test Glicko calculation with submission win."""
    current_rating = 1500
    current_rd = 350
    match_result = MatchResult(winner=True, win_type="SUBMISSION")
    
    new_rating, new_rd = calculate_glicko_rating(current_rating, current_rd, [match_result])
    
    assert new_rating > current_rating  # Should increase for win
    assert new_rd < current_rd  # Should decrease RD

# Integration Tests (25% of coverage)
def test_complete_event_processing():
    """Test processing a complete event from raw data to final output."""
    event_id = "E12692"
    raw_files = ["registrations.csv", "matches.xlsx", "registrations.json"]
    
    result = process_event(event_id, raw_files)
    
    assert result["status"] == "completed"
    assert os.path.exists(f"data/processed/{event_id}_matches.parquet")
    assert os.path.exists("data/datastore/athlete_profiles.json")

# Error Condition Tests (15% of coverage)
def test_invalid_file_format():
    """Test handling of invalid file formats."""
    with pytest.raises(ValueError, match="Invalid file format"):
        process_registration_file("invalid_file.txt")

def test_concurrent_processing_conflict():
    """Test conflict handling for simultaneous processing."""
    task1 = start_data_processing("E12692")
    
    with pytest.raises(ConcurrentOperationError):
        start_data_processing("E12692")
```

### Coverage Tools and Commands
```bash
# Run tests with coverage
pytest --cov=src --cov-report=html --cov-report=term

# Generate coverage report
coverage html

# Check coverage in CI/CD
pytest --cov=src --cov-fail-under=80
```

### Test Data Management
```python
# Use factories for test data
class AthleteFactory:
    @staticmethod
    def create_athlete(**kwargs) -> Athlete:
        defaults = {
            'id': 'A123456',
            'name': 'Test Athlete',
            'age_class': 'adult',
            'skill_level': 'advanced'
        }
        defaults.update(kwargs)
        return Athlete(**defaults)
```

### Performance Testing
- Load test critical endpoints
- Monitor response times under load (target: <30 seconds for API calls)
- Test with realistic data volumes
- Benchmark file I/O operations
- Test concurrent user scenarios
- Verify conflict handling for simultaneous operations
- Test data processing operations (can take several minutes)
- Ensure system remains responsive during long-running tasks



## Monitoring and Observability

### Application Metrics
- Request/response times
- Error rates and types
- File I/O performance
- Memory and CPU usage

### Railway Monitoring Strategy
```python
# Custom metrics for Railway
@app.middleware("http")
async def track_metrics(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # Log custom metrics
    processing_time = time.time() - start_time
    logger.info(
        "Request processed",
        path=request.url.path,
        method=request.method,
        processing_time=processing_time,
        status_code=response.status_code
    )
    
    return response
```

### Railway Deployment Workflow
```bash
# Development → Staging → Production workflow

# Development
git checkout develop
railway up --service dev

# Staging
git checkout main
railway up --service staging

# Production
git tag v0.6.0-alpha.7
git push origin v0.6.0-alpha.7
railway up --service production
```

### Railway-Specific Considerations
- **File Persistence**: Use Railway volumes for persistent data storage
- **Resource Limits**: Memory limits (512MB-8GB), CPU constraints
- **Scaling Strategy**: Start with hobby plan, upgrade to pro for production
- **Backup Strategy**: Implement backup for critical data
- **Ephemeral File System**: Handle Railway's file system limitations

### Logging Standards
```python
import structlog

logger = structlog.get_logger()

def log_athlete_processing(athlete_id: str, event_count: int):
    logger.info(
        "Processing athlete data",
        athlete_id=athlete_id,
        event_count=event_count,
        processing_time=time.time() - start_time
    )
```

### Health Check Endpoints
```python
@app.get("/health")
async def health_check():
    """Railway health check endpoint."""
    try:
        # Check file system access
        file_system_ok = os.access(DATA_DIR, os.W_OK)
        
        # Check memory usage
        memory_ok = psutil.virtual_memory().percent < 90
        
        # Check processing queue
        queue_ok = not is_processing_running()
        
        status = "healthy" if all([file_system_ok, memory_ok, queue_ok]) else "degraded"
        
        return {
            "status": status,
            "timestamp": datetime.utcnow().isoformat(),
            "version": "0.6.0",
            "file_system": file_system_ok,
            "memory_usage": psutil.virtual_memory().percent,
            "processing_queue": queue_ok
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }
```

### Railway-Specific Optimizations
```python
# Railway uses ephemeral file system - need to handle this
import os

# Use Railway's persistent storage
DATA_DIR = os.getenv("RAILWAY_VOLUME_MOUNT_PATH", "./data")

# Ensure directories exist
os.makedirs(f"{DATA_DIR}/raw", exist_ok=True)
os.makedirs(f"{DATA_DIR}/processed", exist_ok=True)
os.makedirs(f"{DATA_DIR}/datastore", exist_ok=True)

# Memory management for Railway limits
def check_memory_usage():
    """Monitor memory usage for Railway limits."""
    memory = psutil.virtual_memory()
    if memory.percent > 80:
        logger.warning(f"High memory usage: {memory.percent}%")
        gc.collect()
```

## Additional Recommendations

### Suggested Tools
- **Pre-commit**: Automated code quality checks
- **Black**: Code formatting
- **isort**: Import sorting
- **flake8**: Linting
- **mypy**: Type checking
- **pytest**: Testing framework
- **coverage**: Test coverage reporting
- **locust**: Load testing
- **Railway CLI**: Railway deployment and management
- **Railway monitoring**: Built-in monitoring and alerting
- **psutil**: System monitoring for Railway constraints

### IDE Configuration
- Enable type checking
- Configure linting rules
- Set up debugging configurations
- Use consistent formatting settings
- Enable auto-formatting on save

### Team Collaboration
- Use pull request reviews
- Maintain coding standards
- Share knowledge and best practices
- Regular code reviews and feedback
- Pair programming for complex features

### Version Control Best Practices
- Use feature branches for all development
- Require code reviews before merging
- Use conventional commit messages
- Tag releases with semantic versioning
- Keep commit history clean and linear

This document should be updated as the project evolves and new requirements are identified. 